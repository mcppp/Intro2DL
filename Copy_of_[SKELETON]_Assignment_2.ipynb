{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [SKELETON] Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f6b16456bec941559b01b2a8de44fb3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_84c26ad7f63c4aea9f1630397f07318e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_867a5dd9b8ad47eb9cf4f380fcf8573a",
              "IPY_MODEL_115a201a76074a35b8d5a4dbdff2eb08"
            ]
          }
        },
        "84c26ad7f63c4aea9f1630397f07318e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "867a5dd9b8ad47eb9cf4f380fcf8573a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0469c4dafd7c4b4090ce67219e01db69",
            "_dom_classes": [],
            "description": "  7%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 118784,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8409,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed3dae778cfe4422a3425117ca157a08"
          }
        },
        "115a201a76074a35b8d5a4dbdff2eb08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c8278f4c0a5243a3afd0f8a3b37fb523",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8409/118784 [12:24&lt;2:38:05, 11.64it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aa4902a50b6e486e8ccea5648dfc7efe"
          }
        },
        "0469c4dafd7c4b4090ce67219e01db69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed3dae778cfe4422a3425117ca157a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c8278f4c0a5243a3afd0f8a3b37fb523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aa4902a50b6e486e8ccea5648dfc7efe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcppp/Intro2DL/blob/main/Copy_of_%5BSKELETON%5D_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HplGJuJR01Rl"
      },
      "source": [
        "### USE ONLY THESE PACKAGES ###\r\n",
        "import os\r\n",
        "import csv\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrfsqemms0rN"
      },
      "source": [
        "### Upload the lyrics_dataset.zip in the colab's folder first !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MuSRKIVGyYR",
        "outputId": "109a6a10-e3f3-4fef-9e70-2854af2c18ca"
      },
      "source": [
        "!unzip lyrics_dataset.zipA"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  lyrics_dataset.zip\n",
            "replace lyrics_dataset/adele.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: lyrics_dataset/adele.txt  \n",
            "  inflating: lyrics_dataset/al-green.txt  \n",
            "  inflating: lyrics_dataset/alicia-keys.txt  \n",
            "  inflating: lyrics_dataset/amy-winehouse.txt  \n",
            "  inflating: lyrics_dataset/beatles.txt  \n",
            "  inflating: lyrics_dataset/bieber.txt  \n",
            "  inflating: lyrics_dataset/bjork.txt  \n",
            "  inflating: lyrics_dataset/blink-182.txt  \n",
            "  inflating: lyrics_dataset/bob-dylan.txt  \n",
            "  inflating: lyrics_dataset/bob-marley.txt  \n",
            "  inflating: lyrics_dataset/britney-spears.txt  \n",
            "  inflating: lyrics_dataset/bruce-springsteen.txt  \n",
            "  inflating: lyrics_dataset/bruno-mars.txt  \n",
            "  inflating: lyrics_dataset/cake.txt  \n",
            "  inflating: lyrics_dataset/dickinson.txt  \n",
            "  inflating: lyrics_dataset/disney.txt  \n",
            "  inflating: lyrics_dataset/dj-khaled.txt  \n",
            "  inflating: lyrics_dataset/dolly-parton.txt  \n",
            "  inflating: lyrics_dataset/dr-seuss.txt  \n",
            "  inflating: lyrics_dataset/drake.txt  \n",
            "  inflating: lyrics_dataset/eminem.txt  \n",
            "  inflating: lyrics_dataset/janisjoplin.txt  \n",
            "  inflating: lyrics_dataset/jimi-hendrix.txt  \n",
            "  inflating: lyrics_dataset/johnny-cash.txt  \n",
            "  inflating: lyrics_dataset/joni-mitchell.txt  \n",
            "  inflating: lyrics_dataset/Kanye_West.txt  \n",
            "  inflating: lyrics_dataset/lady-gaga.txt  \n",
            "  inflating: lyrics_dataset/leonard-cohen.txt  \n",
            "  inflating: lyrics_dataset/Lil_Wayne.txt  \n",
            "  inflating: lyrics_dataset/lin-manuel-miranda.txt  \n",
            "  inflating: lyrics_dataset/lorde.txt  \n",
            "  inflating: lyrics_dataset/ludacris.txt  \n",
            "  inflating: lyrics_dataset/michael-jackson.txt  \n",
            "  inflating: lyrics_dataset/missy-elliott.txt  \n",
            "  inflating: lyrics_dataset/nickelback.txt  \n",
            "  inflating: lyrics_dataset/nicki-minaj.txt  \n",
            "  inflating: lyrics_dataset/nirvana.txt  \n",
            "  inflating: lyrics_dataset/notorious-big.txt  \n",
            "  inflating: lyrics_dataset/nursery_rhymes.txt  \n",
            "  inflating: lyrics_dataset/patti-smith.txt  \n",
            "  inflating: lyrics_dataset/paul-simon.txt  \n",
            "  inflating: lyrics_dataset/prince.txt  \n",
            "  inflating: lyrics_dataset/r-kelly.txt  \n",
            "  inflating: lyrics_dataset/radiohead.txt  \n",
            "  inflating: lyrics_dataset/rihanna.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfJnnXc7O58m"
      },
      "source": [
        "### Do not change this seed number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcZM4dCTpfz7",
        "outputId": "0a66ddc4-7fe0-4838-8ebb-e83f72b35171"
      },
      "source": [
        "np.random.seed(1)\r\n",
        "torch.manual_seed(1)\r\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f49f1fdfb58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fUfhvwLvAMf"
      },
      "source": [
        "## Use to construct dataset and dataloader!!!!\r\n",
        "## Use below code to train your model with all lyrics\r\n",
        "lyrics = list()\r\n",
        "for txt_file in os.listdir('./lyrics_dataset'):\r\n",
        "    if txt_file[0] != '.':\r\n",
        "        target_txt = os.path.join('./lyrics_dataset', txt_file)\r\n",
        "        f = open(target_txt, 'r')\r\n",
        "        curr_lyrics = f.readlines()\r\n",
        "        for i in range(len(curr_lyrics)):\r\n",
        "            curr_lyrics[i] = curr_lyrics[i].lower()\r\n",
        "        curr_lyrics = list(set(curr_lyrics))\r\n",
        "        lyrics += curr_lyrics\r\n",
        "# print(len(lyrics)) #how many lines\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#1: deleting everything but alphanumerics and blank spaces\r\n",
        "lyrics1=lyrics[:50]  #for short example\r\n",
        "stripped = []\r\n",
        "for i in lyrics1:\r\n",
        "  stripped.append ( ''.join(ch for ch in i if ch.isalnum() or ch == ' ').split())\r\n",
        "\r\n",
        "# a = list()\r\n",
        "# a = stripped\r\n",
        "\r\n",
        "# # type(a[0]) #checking that self.trimmed_lyrics is a list with lists inside of it\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# #2: making vocabulary of unique words\r\n",
        "# unique = list()\r\n",
        "# for x in a:  \r\n",
        "#     for i in x:\r\n",
        "#       if i not in unique:\r\n",
        "#         unique.append(i)\r\n",
        "# # print(unique)\r\n",
        "\r\n",
        "\r\n",
        "# ##3: freq dict\r\n",
        "# frequency = dict()\r\n",
        "# for x in a:\r\n",
        "#   for i in x:\r\n",
        "#       if i not in frequency:\r\n",
        "#         frequency[i] = 0 \r\n",
        "#       frequency[i] += 1\r\n",
        "# # print(frequency)\r\n",
        "\r\n",
        "# ##4:\r\n",
        "# idx=4 #which sentence we want\r\n",
        "# target_lyrics = a[idx] #chosen sentence separated in words\r\n",
        "# print(target_lyrics)\r\n",
        "# # print(len(target_lyrics))\r\n",
        "# target_word = np.random.choice(target_lyrics)\r\n",
        "\r\n",
        "# # random_iterator = iter(target_lyrics) #creating iterator\r\n",
        "# # target_word = next(random_iterator) #randomly choose target word in sentence\r\n",
        "\r\n",
        "# target_idx = target_lyrics.index(target_word) #index of target word\r\n",
        "# # print(target_idx)\r\n",
        "\r\n",
        "# sample = dict()\r\n",
        "# sample['pairs'] = list()\r\n",
        "\r\n",
        "# ###CHOSEN MANUALLY TO TEST\r\n",
        "# target_idx =3\r\n",
        "# target_word=target_lyrics[target_idx]\r\n",
        "\r\n",
        "# len_sentence = len(target_lyrics)\r\n",
        "# print(len_sentence)\r\n",
        "\r\n",
        "# window = 2\r\n",
        "# window_len = list(range(1, window+1, 1))\r\n",
        "# print(window_len)\r\n",
        "\r\n",
        "# # for i in window_len:\r\n",
        "# #   if (target_idx+i)<=(len_sentence-1):\r\n",
        "# #     sample['pairs'].append([target_idx,target_idx + i])\r\n",
        "# #   if (target_idx - i) >= 0:\r\n",
        "# #     sample['pairs'].append([target_idx, target_idx - i])\r\n",
        "\r\n",
        "# # print(sample['pairs'])\r\n",
        "# print(a)\r\n",
        "# print(unique)\r\n",
        "\r\n",
        "# for i in window_len:\r\n",
        "#   if (target_idx+i)<=(len_sentence-1):\r\n",
        "#     orig_idx = unique.index(target_word)\r\n",
        "#     next_idx = unique.index((target_lyrics[target_idx + i]))\r\n",
        "#     sample['pairs'].append((orig_idx,next_idx))\r\n",
        "#   if (target_idx - i) >= 0:\r\n",
        "#     orig1_idx = unique.index(target_word)\r\n",
        "#     next1_idx = unique.index((target_lyrics[target_idx - i]))\r\n",
        "#     sample['pairs'].append((orig1_idx,next1_idx))\r\n",
        "\r\n",
        "# print(sample['pairs'])\r\n",
        "\r\n",
        "\r\n",
        "# # sample['pairs'][0] = (2,1)\r\n",
        "\r\n",
        "# # target_idx =1\r\n",
        "# # target_word = target_lyrics[target_idx]\r\n",
        "# # print(target_word)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# # Use below code to train your model with specific artist's lyrics\r\n",
        "# target_txt = './lyrics_dataset/lady-gaga.txt'\r\n",
        "# f = open(target_txt, 'r')\r\n",
        "# lyrics = f.readlines()\r\n",
        "# for i in range(len(lyrics)):\r\n",
        "#     lyrics[i] = lyrics[i].lower()\r\n",
        "# lyrics = list(set(lyrics))\r\n",
        "\r\n",
        "# # #\"lyrics\" is a list of strings.\r\n",
        "# for l in lyrics:\r\n",
        "#     print(l)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuH1d2SKvjmZ",
        "outputId": "b5c70a63-9c55-42be-b83c-962cd94fcc47"
      },
      "source": [
        "# #HELP EXAMPLES\r\n",
        "\r\n",
        "# print(lyrics[103])\r\n",
        "# trimmed_lyrics = list()\r\n",
        "# trimmed_lyrics.append(['chola', 'or', 'orient', 'made'])\r\n",
        "\r\n",
        "# trimmed_lyrics.append(['me', 'or', 'you'])\r\n",
        "# # print(trimmed_lyrics) #how we want it to look like (eliminate symbols too)\r\n",
        "# # vocabulary= ['chola', 'or', 'orient', 'made','me', 'you'] #vocab list with unique words\r\n",
        "\r\n",
        "# # frequency=dict() #how many times a word appears\r\n",
        "# # frequency['chola'] = 1\r\n",
        "# # frequency['or'] = 2\r\n",
        "# # frequency['orient'] = 1\r\n",
        "# # frequency['made'] = 1\r\n",
        "# # frequency['me'] = 1\r\n",
        "# # frequency['you'] = 1\r\n",
        "# # print(frequency)\r\n",
        "\r\n",
        "# #MUST DO 4\r\n",
        "# print(trimmed_lyrics[0])  #first list of words\r\n",
        "# target_lyrics = trimmed_lyrics[0]\r\n",
        "# targed_idx =1\r\n",
        "# target_word = target_lyrics[target_idx]\r\n",
        "# print(target_word)\r\n",
        "# vocabulary= ['chola', 'or', 'orient', 'made','me', 'you'] #vocab list with unique words\r\n",
        "#     #          0        1       2        3     4     5\r\n",
        "# sample = dict()\r\n",
        "# sample['pairs'] = list()\r\n",
        "# sample['pairs'] = [(1,2),(1,0)] #[('or','orient'),('or', 'chola')] #  needs to be converted into integer\r\n",
        "#                  #(targe word's index, context word's index)\r\n",
        "# #your code here\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "but she got that dumb ass\n",
            "\n",
            "['chola', 'or', 'orient', 'made']\n",
            "made\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGJNDVzh9Bny"
      },
      "source": [
        "#we wanna get lyrics and eliminate commas, brackets, etc.\r\n",
        "\r\n",
        "class lyricsDataset(Dataset): # 12 points\r\n",
        "    def __init__(self, lyrics, window_len):\r\n",
        "        self.lyrics = lyrics\r\n",
        "        self.window_len = window_len #how many words we see before and after target word\r\n",
        "        # window len means, that, if i-th word is target\r\n",
        "        # you would consider i-window_len ~ i+window_len words as context words\r\n",
        "\r\n",
        "        ##--------------write below-------------## <--- do not erase this afterwards\r\n",
        "        self.trimmed_lyrics = list()    \r\n",
        "        self.vocabulary = list()\r\n",
        "        self.frequency = dict()\r\n",
        "        # your code in here\r\n",
        "        \r\n",
        "        #1: Eliminating everything but alphanumerics and blank spaces + splitting by blank spaces\r\n",
        "        stripped = []\r\n",
        "        for i in lyrics:\r\n",
        "          stripped.append ( ''.join(ch for ch in i if ch.isalnum() or ch == ' ').split())\r\n",
        "        self.trimmed_lyrics = stripped\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        #2: Making vocabulary of unique words\r\n",
        "        unique = list()\r\n",
        "        for x in self.trimmed_lyrics:  \r\n",
        "          for i in x:\r\n",
        "            if i not in unique:\r\n",
        "              unique.append(i)\r\n",
        "        self.vocabulary = unique\r\n",
        "\r\n",
        "        #3: Creating frequency dict\r\n",
        "        for x in self.trimmed_lyrics:\r\n",
        "          for i in x:\r\n",
        "            if i not in self.frequency:\r\n",
        "              self.frequency[i] = 0 \r\n",
        "            self.frequency[i] += 1\r\n",
        "        \r\n",
        "    \r\n",
        "        ##--------------write above-------------## <--- do not erase this\r\n",
        "        \r\n",
        "        ### MUST TO-DO 1. (+3) -> self.trimmed_lyrics\r\n",
        "        # for strings in self.lyrics, \r\n",
        "        # (+2) write a code to eliminate every character except for alphabet, number, and space (\" \")\r\n",
        "        ### for example, - + ? ! ' \" [ ] ( ) <- char like this should be excluded. \r\n",
        "        # (+1) after that, split each string with respect to the space.\r\n",
        "        ### If proprocessed string is \"hello hello hello\", \r\n",
        "        ### a list ['hello', 'hello', 'hello'] should be generated.\r\n",
        "        ### Then, put that list into the self.trimmed_lyrics.\r\n",
        "        ### self.trimmed_lyrics needs to be a LIST which has LIST as element.\r\n",
        "\r\n",
        "        ### MUST TO-DO 2. (+2) -> self.vocabulary\r\n",
        "        # (+2) Put all words(string) in self.trimmed_lyrics to the self.vocabulary. \r\n",
        "        ### In self.vocabulary, each word needs to be unique.\r\n",
        "        ### which means, this list (self.vocabulary) should not have duplicated elements. \r\n",
        "        ### self.vocabulary needs to be a LIST which contains unique words in self.trimmed_lyrics\r\n",
        "        ### If your code contains duplicated words, you would not get the point.\r\n",
        "        ### If there is a word that are neglected from self.trimmed_lyrics, you would not get the point.\r\n",
        "\r\n",
        "        ### MUST TO-DO 3. (+2) -> self.frequency\r\n",
        "        # (+2) In self.frequency, Record how many times each word in self.vocabulary \r\n",
        "        ###                              appears in self.trimmed_lyrics.\r\n",
        "        ### For example, if \"love\" appears 100 times in self.trimmed_lyrics,\r\n",
        "        ### it should be: self.frequency[\"love\"] = 100\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        # DO NOT TOUCH BELOW. JUST USE BELOW CODE FOR YOUR __len__ \r\n",
        "        return len(self.trimmed_lyrics)\r\n",
        "        # DO NOT TOUCH ABOVE. JUST USE ABOVE CODE FOR YOUR __len__ \r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        ### MUST TO-DO 4. (+5) --> sample (bring out positive samples from lyrics inside this dataset)\r\n",
        "        ##--------------write below-------------##\r\n",
        "        sample = dict()\r\n",
        "        sample['pairs'] = list()\r\n",
        "        \r\n",
        "        # your code here\r\n",
        "\r\n",
        "        ##Q : recheck if next () works well\r\n",
        "              \r\n",
        "        # random_iterator = iter(self.trimmed_lyrics[idx]) #creating iterator\r\n",
        "        # target_word = next(random_iterator) #randomly choose target word in sentence\r\n",
        "\r\n",
        "        if (len(self.trimmed_lyrics[idx])<1):\r\n",
        "          return sample\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "        target_word = np.random.choice(self.trimmed_lyrics[idx])\r\n",
        "        # print('Target word: ',target_word)\r\n",
        "        target_idx = self.trimmed_lyrics[idx].index(target_word) #index of target word  \r\n",
        "               \r\n",
        "        \r\n",
        "        len_sentence = len(self.trimmed_lyrics[idx]) #check how many words in selected line (sentence)\r\n",
        "        \r\n",
        "        window_len = list(range(1, self.window_len+1, 1)) #range of window lengths to check   \r\n",
        "\r\n",
        "           \r\n",
        "        for i in window_len:\r\n",
        "          if (target_idx+i)<=(len_sentence-1):\r\n",
        "            orig_idx = self.vocabulary.index(target_word)\r\n",
        "            next_idx = self.vocabulary.index((self.trimmed_lyrics[idx][target_idx + i]))\r\n",
        "            sample['pairs'].append((orig_idx,next_idx))\r\n",
        "          if (target_idx - i) >= 0:\r\n",
        "            orig1_idx = self.vocabulary.index(target_word)\r\n",
        "            next1_idx = self.vocabulary.index((self.trimmed_lyrics[idx][target_idx - i]))\r\n",
        "            sample['pairs'].append((orig1_idx,next1_idx))\r\n",
        "          \r\n",
        "        # print(sample['pairs'])\r\n",
        "        return sample\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        "
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8htM2GCAyKhu",
        "outputId": "9254563b-e231-4bdd-da61-494b8c3b01c9"
      },
      "source": [
        "a = []\r\n",
        "b = ('hola','hi')\r\n",
        "print(len(a))\r\n",
        "print(len(b))\r\n",
        "\r\n",
        "c = np.random.choice(b, size = int(len(b)<=1))\r\n",
        "print(c)\r\n",
        "d = np.random.choice(a, size =0)\r\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzSkbpoFQnv3"
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES.\r\n",
        "### PENALTY (-5) CAN BE APPLIED IF YOUR CODE DOES NOT WORK FOR VARIOUS VALUES OF WINDOW_LEN\r\n",
        "dataset = lyricsDataset(lyrics, 3)\r\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\r\n",
        "### DO NOT TOUCH ABOVE. JUST USE THESE LINES."
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeBtmoaymR0o",
        "outputId": "b5273d3c-e094-4276-b162-e3149a7b74df"
      },
      "source": [
        "example = next(iter(dataloader)) #getting one example from dataloader\r\n",
        "example1 = next(iter(dataloader))\r\n",
        "print(example['pairs'])\r\n",
        "print(example1['pairs'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[tensor([1318]), tensor([2578])], [tensor([1318]), tensor([24])], [tensor([1318]), tensor([78])], [tensor([1318]), tensor([547])]]\n",
            "[[tensor([151]), tensor([7544])], [tensor([151]), tensor([162])], [tensor([151]), tensor([9824])], [tensor([151]), tensor([9])]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5WbDFRf9yzI"
      },
      "source": [
        "class Word2Vec(nn.Module):# 15 points\r\n",
        "  \r\n",
        "    def __init__(self, num_vocabs, embed_dim = 300): ## do not change 'embed_dim' value.\r\n",
        "        ### MUST TO-DO 5 : (+5)\r\n",
        "        ##--------------write below-------------##\r\n",
        "        # your code here\r\n",
        "        super(Word2Vec, self).__init__()  #needed so that things defined in nn.Module are inherited by our class\r\n",
        "\r\n",
        "        #define sizes of network\r\n",
        "        self.num_vocabs = num_vocabs\r\n",
        "        self.embed_dim = embed_dim\r\n",
        "         #define layers of network\r\n",
        "        self.layer1 = nn.Linear(self.num_vocabs, self.embed_dim)\r\n",
        "        self.layer2 = nn.Linear(self.num_vocabs, self.embed_dim)       \r\n",
        "\r\n",
        "\r\n",
        "        ##--------------write above-------------##\r\n",
        "        # Define a model which can map word's integer index value to word embedding.\r\n",
        "        \r\n",
        "    def forward(self, pairs):\r\n",
        "      '''\r\n",
        "      input : pairs - one tuple of (target word index, context word index)\r\n",
        "      '''\r\n",
        "      ### MUST TO DO 6 : (+10)\r\n",
        "        ##--------------write below-------------##\r\n",
        "        # your code here\r\n",
        "      # print('passed pair: ', pairs)\r\n",
        "      # print('num_vocabs: ', self.num_vocabs)\r\n",
        "      #hot encoding input words (target + context)\r\n",
        "      target_idx = pairs[0].long() #corresponding idx of target word from vocab\r\n",
        "      context_idx = pairs[1].long() #corresponding idx of context word from vocab\r\n",
        " \r\n",
        "      \r\n",
        "      one_hot_target = torch.zeros(self.num_vocabs)\r\n",
        "      # one_hot_target = np.zeros(self.num_vocabs)\r\n",
        "      one_hot_target[target_idx] = 1 #now target word hot encoded\r\n",
        "        \r\n",
        "      one_hot_context = torch.zeros(self.num_vocabs)\r\n",
        "      # one_hot_context = np.zeros(self.num_vocabs)\r\n",
        "      one_hot_context[context_idx] = 1 #now context word hot encoded\r\n",
        "\r\n",
        "\r\n",
        "      \r\n",
        "      #layers\r\n",
        "      # print('one_hot_target.shape: ', one_hot_target.shape)\r\n",
        "      target_embed = self.layer1(one_hot_target)\r\n",
        "      # print('target_embed.shape: ',target_embed.shape)\r\n",
        "      # print('one_hot_context.shape: ',one_hot_context.shape)\r\n",
        "      context_embed = self.layer2(one_hot_context) \r\n",
        "      # print('context_embed.shape: ',context_embed.shape)\r\n",
        "      \r\n",
        "\r\n",
        "      \r\n",
        "\r\n",
        "      return target_embed, context_embed\r\n",
        "        ##--------------write above-------------##\r\n",
        "        # return the embedding of target word and context word.\r\n",
        "        # i.e., return target_embed, context_embed\r\n"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_djd4UufFd9"
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK WITH THESE LINES\r\n",
        "model = Word2Vec(len(dataset.vocabulary)) #the arg is num_vocabulary\r\n",
        "optimizer = torch.optim.AdamW(model.parameters())\r\n",
        "# device = 'cuda'\r\n",
        "\r\n",
        "### DO NOT TOUCH ABOVE. JUST USE THESE LINES. MAKE YOUR CODE WORK WITH THESE LINES\r\n",
        "### USE THIS GIVEN OPTIMIZER."
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iyCIBNqF3UO",
        "outputId": "df345e2e-3432-4a64-9348-b4f29cc385a5"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(\n",
            "  (layer1): Linear(in_features=30298, out_features=300, bias=True)\n",
            "  (layer2): Linear(in_features=30298, out_features=300, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLY1vFx21wd6",
        "outputId": "3a82a209-2a4a-4399-f5e6-9a12e7032069"
      },
      "source": [
        "out = model(example['pairs'][0])\r\n",
        "print(len(out)) #target_embed and context_embed returned"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "passed pair:  [tensor([72]), tensor([754])]\n",
            "num_vocabs:  30298\n",
            "one_hot_target.shape:  torch.Size([30298])\n",
            "target_embed.shape:  torch.Size([300])\n",
            "one_hot_context.shape:  torch.Size([30298])\n",
            "context_embed.shape:  torch.Size([300])\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiO9d05dwZzp"
      },
      "source": [
        "#Creating negative samples \r\n",
        "\r\n",
        "nsample =dict()\r\n",
        "nsample['pairs']=list()\r\n",
        "nsample1 =dict()\r\n",
        "nsample1['pairs'] = list()\r\n",
        "\r\n",
        "nembeddings1= dict()\r\n",
        "nembeddings1['pairs']=list()\r\n",
        "nembeddings1['dot']=list()\r\n",
        "\r\n",
        "nembeddings2= dict()\r\n",
        "nembeddings2['pairs']=list()\r\n",
        "nembeddings2['dot']=list()\r\n",
        "\r\n",
        "denom =list()\r\n",
        "\r\n",
        "# print(len(example1['pairs']))\r\n",
        "# print(len(example['pairs']))\r\n",
        "# print(example1['pairs'])\r\n",
        "# print(example['pairs'])\r\n",
        "\r\n",
        "for idx, j in enumerate(example1['pairs']):\r\n",
        "  # print((example['pairs'][0][0], example1['pairs'][idx][1]))\r\n",
        "  nsample1['pairs'].append((example['pairs'][0][0], example1['pairs'][idx][1]))\r\n",
        "\r\n",
        "for idx, i in enumerate(example['pairs']):\r\n",
        "  nsample['pairs'].append((example1['pairs'][0][0], example['pairs'][idx][1]))\r\n",
        "\r\n",
        "# for idx, i in enumerate(nsample1['pairs']):\r\n",
        "#       target_embed, context_embed = model(nsample1['pairs'][idx])\r\n",
        "#       nembeddings1['pairs'].append((target_embed,context_embed))\r\n",
        "#       target_embed=target_embed.detach().numpy()\r\n",
        "#       context_embed=context_embed.detach().numpy()\r\n",
        "#       nembeddings1['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "#       denom.append(np.dot(target_embed, context_embed))\r\n",
        "\r\n",
        "# for idx, i in enumerate(nsample['pairs']):\r\n",
        "#       target_embed, context_embed = model(nsample['pairs'][idx])\r\n",
        "#       nembeddings2['pairs'].append((target_embed,context_embed))\r\n",
        "#       target_embed=target_embed.detach().numpy()\r\n",
        "#       context_embed=context_embed.detach().numpy()\r\n",
        "#       nembeddings2['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "#       denom.append(np.dot(target_embed, context_embed))\r\n",
        "\r\n",
        "\r\n",
        "# print(nembeddings1['dot'])\r\n",
        "# print(nembeddings2['dot'])\r\n",
        "# print(denom)\r\n",
        "\r\n",
        "# print(np.exp(denom))\r\n",
        "# d = np.sum(np.exp(denom))\r\n",
        "# print(d)\r\n",
        "\r\n",
        "# prob = np.divide((np.exp(denom)), d)\r\n",
        "# print(prob)\r\n",
        "\r\n",
        "# y = np.ones(8,int)\r\n",
        "# m = y.shape[0]\r\n",
        "\r\n",
        "# part = np.multiply(y, -np.log(prob))\r\n",
        "# loss = np.sum(part) / m\r\n",
        "\r\n",
        "# a = list()\r\n",
        "\r\n",
        "# a.append(0)\r\n",
        "# a.append (2)\r\n",
        "# print(a)\r\n",
        "\r\n",
        "# print (a)\r\n",
        "\r\n",
        "# print(example['pairs'])\r\n",
        "# print(example1['pairs'])\r\n",
        "# print()\r\n",
        "# print(nsample['pairs'])\r\n",
        "# print(nsample1['pairs'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rp8hvPWjHxe"
      },
      "source": [
        "def train(model, optimizer, sample_1, sample_2): # 13 points\r\n",
        "    ### MUST TO DO 6 : (+12)\r\n",
        "    ##--------------write below-------------##\r\n",
        "    model.train()\r\n",
        "    # model = model.to(device)\r\n",
        "    optimizer.zero_grad()\r\n",
        "    # your code here\r\n",
        "\r\n",
        "    # sample_1['pairs']=torch.Tensor(sample_1['pairs']).to(device)\r\n",
        "    # sample_2['pairs']=torch.Tensor(sample_2['pairs']).to(device)\r\n",
        "\r\n",
        "    ##deleting empty lines - Option 2\r\n",
        "    if (len(sample_1['pairs'])<=1 or len(sample_2['pairs'])<=1 ):\r\n",
        "      loss = 0\r\n",
        "      return loss\r\n",
        "\r\n",
        "    # #deleting empty lines - Option 1\r\n",
        "    # while (len(sample_1['pairs'])<1):\r\n",
        "    #   sample_1 = next(iter(dataloader))\r\n",
        "    # while (len(sample_2['pairs'])<1):\r\n",
        "    #   sample_2 = next(iter(dataloader))\r\n",
        "\r\n",
        "\r\n",
        "    #creating negative samples\r\n",
        "    nsample_1 = dict()\r\n",
        "    nsample_1['pairs']=list()\r\n",
        "    nsample_2 =dict()\r\n",
        "    nsample_2['pairs']=list()\r\n",
        "\r\n",
        "    # print(sample_1['pairs'])\r\n",
        "    # print(len(sample_1['pairs']))\r\n",
        "    # print(sample_2['pairs'])\r\n",
        "    # print(len(sample_2['pairs']))\r\n",
        "\r\n",
        "    for idx, j in enumerate(sample_1['pairs']):\r\n",
        "      nsample_2['pairs'].append((sample_2['pairs'][0][0], sample_1['pairs'][idx][1]))\r\n",
        "\r\n",
        "    for idx, i in enumerate(sample_2['pairs']):\r\n",
        "      nsample_1['pairs'].append((sample_1['pairs'][0][0], sample_2['pairs'][idx][1]))\r\n",
        "\r\n",
        "\r\n",
        "    #applying embedding to all tuples in the samples\r\n",
        "    # target_embed, context_embed = model((target, context))\r\n",
        "    embeddings_1 = dict()\r\n",
        "    embeddings_2 = dict()\r\n",
        "    nembeddings_1 = dict()\r\n",
        "    nembeddings_2 = dict()\r\n",
        "\r\n",
        "    embeddings_1['pairs'] = list()\r\n",
        "    embeddings_2['pairs'] = list()\r\n",
        "    nembeddings_1['pairs'] = list()\r\n",
        "    nembeddings_2['pairs'] = list()\r\n",
        "    embeddings_1['dot'] = list()\r\n",
        "    embeddings_2['dot'] = list()\r\n",
        "    nembeddings_1['dot'] = list()\r\n",
        "    nembeddings_2['dot'] = list()\r\n",
        "\r\n",
        "    denominator = list()\r\n",
        "    labels = list()\r\n",
        "    \r\n",
        "    for idx, i in enumerate(sample_1['pairs']):\r\n",
        "      target_embed, context_embed = model(sample_1['pairs'][idx])\r\n",
        "      embeddings_1['pairs'].append((target_embed,context_embed))\r\n",
        "      target_embed=target_embed.detach().numpy()\r\n",
        "      context_embed=context_embed.detach().numpy()\r\n",
        "      embeddings_1['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "      denominator.append(np.dot(target_embed, context_embed))\r\n",
        "      labels.append(1)\r\n",
        "\r\n",
        "    for idx, i in enumerate(sample_2['pairs']):\r\n",
        "      target_embed, context_embed = model(sample_2['pairs'][idx])\r\n",
        "      embeddings_2['pairs'].append((target_embed,context_embed))\r\n",
        "      target_embed=target_embed.detach().numpy()\r\n",
        "      context_embed=context_embed.detach().numpy()\r\n",
        "      embeddings_2['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "      denominator.append(np.dot(target_embed, context_embed))\r\n",
        "      labels.append(1)\r\n",
        "     \r\n",
        "\r\n",
        "    for idx, i in enumerate(nsample_1['pairs']):\r\n",
        "      target_embed, context_embed = model(nsample_1['pairs'][idx])\r\n",
        "      nembeddings_1['pairs'].append((target_embed,context_embed))\r\n",
        "      target_embed=target_embed.detach().numpy()\r\n",
        "      context_embed=context_embed.detach().numpy()\r\n",
        "      nembeddings_1['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "      denominator.append(np.dot(target_embed, context_embed))\r\n",
        "      labels.append(0)\r\n",
        "\r\n",
        "    for idx, i in enumerate(nsample_2['pairs']):\r\n",
        "      target_embed, context_embed = model(nsample_2['pairs'][idx])\r\n",
        "      nembeddings_2['pairs'].append((target_embed,context_embed))\r\n",
        "      target_embed=target_embed.detach().numpy()\r\n",
        "      context_embed=context_embed.detach().numpy()\r\n",
        "      nembeddings_2['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "      denominator.append(np.dot(target_embed, context_embed))\r\n",
        "      labels.append(0)\r\n",
        "\r\n",
        "\r\n",
        "    # for idx, i in enumerate(sample_1['pairs']):\r\n",
        "    #   target_embed, context_embed = model(sample_1['pairs'][idx][0].float().to(device), sample_1['pairs'][idx][1].float().to(device))\r\n",
        "    #   embeddings_1['pairs'].append((target_embed,context_embed))\r\n",
        "    #   target_embed=target_embed.detach().numpy()\r\n",
        "    #   context_embed=context_embed.detach().numpy()\r\n",
        "    #   embeddings_1['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "    #   denominator.append(np.dot(target_embed, context_embed))\r\n",
        "    #   labels.append(1)\r\n",
        "\r\n",
        "    # for idx, i in enumerate(sample_2['pairs']):\r\n",
        "    #   target_embed, context_embed = model(sample_2['pairs'][idx][0].float().to(device), sample_2['pairs'][idx][1].float().to(device))\r\n",
        "    #   embeddings_2['pairs'].append((target_embed,context_embed))\r\n",
        "    #   target_embed=target_embed.detach().numpy()\r\n",
        "    #   context_embed=context_embed.detach().numpy()\r\n",
        "    #   embeddings_2['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "    #   denominator.append(np.dot(target_embed, context_embed))\r\n",
        "    #   labels.append(1)\r\n",
        "     \r\n",
        "\r\n",
        "    # for idx, i in enumerate(nsample_1['pairs']):\r\n",
        "    #   target_embed, context_embed = model(nsample_1['pairs'][idx][0].float().to(device), nsample_1['pairs'][idx][1].float().to(device))\r\n",
        "    #   nembeddings_1['pairs'].append((target_embed,context_embed))\r\n",
        "    #   target_embed=target_embed.detach().numpy()\r\n",
        "    #   context_embed=context_embed.detach().numpy()\r\n",
        "    #   nembeddings_1['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "    #   denominator.append(np.dot(target_embed, context_embed))\r\n",
        "    #   labels.append(0)\r\n",
        "\r\n",
        "    # for idx, i in enumerate(nsample_2['pairs']):\r\n",
        "    #   target_embed, context_embed = model(nsample_2['pairs'][idx][0].float().to(device), nsample_2['pairs'][idx][1].float().to(device))\r\n",
        "    #   nembeddings_2['pairs'].append((target_embed,context_embed))\r\n",
        "    #   target_embed=target_embed.detach().numpy()\r\n",
        "    #   context_embed=context_embed.detach().numpy()\r\n",
        "    #   nembeddings_2['dot'].append(np.dot(target_embed, context_embed))\r\n",
        "    #   denominator.append(np.dot(target_embed, context_embed))\r\n",
        "    #   labels.append(0)\r\n",
        "\r\n",
        "    \r\n",
        "    # labels = labels.long().to(device)\r\n",
        "\r\n",
        "    \r\n",
        "    # print('enominator ',denominator)\r\n",
        "    # print('labels ',labels)\r\n",
        "\r\n",
        "    #probability formula\r\n",
        "    denom = np.sum(np.exp(denominator)) #summation of exponential scores\r\n",
        "    # print('denom ', denom)\r\n",
        "    \r\n",
        "    prob = np.divide((np.exp(denominator)), denom) #probability elementwise\r\n",
        "\r\n",
        "    # #Option1    \r\n",
        "    # m = len(labels) #how many labels (how many elements)\r\n",
        "    # part = np.multiply(labels, -np.log(prob))\r\n",
        "    # loss = np.sum(part) / m\r\n",
        "    \r\n",
        "\r\n",
        "    #Option2\r\n",
        "    #https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\r\n",
        "    loss = []\r\n",
        "    for idx, i in enumerate(labels):\r\n",
        "      if  labels[idx]== 1:\r\n",
        "        loss.append(-np.log(prob[idx]))\r\n",
        "      else:\r\n",
        "        loss.append(-np.log(1 - prob[idx]))\r\n",
        "\r\n",
        "    loss = np.sum(loss) / len(labels)\r\n",
        "\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    return loss\r\n",
        "    # return the current loss value\r\n",
        "    ##--------------write above-------------##\r\n",
        "\r\n",
        "    ## sample_1 will contain positive (target_1, context_1) tuples\r\n",
        "    ## sample_2 will contain another positive (target_2, context_2) tuples\r\n",
        "    ## But for the negative sampling, we need negative (target, context) tuples.\r\n",
        "    ## NEGATIVE TUPLES can be generated by (target_1, context_2), (target_2, context_1)\r\n",
        "    ## (+5) Generate and give positive & negative tuples for model's input\r\n",
        "    ##### Then, you would get : target_embed, context_embed = model((target, context)).\r\n",
        "    ## (+4) calculate the distance between target_embed and context_embed by DOT PRODUCT  #see slides\r\n",
        "    ## (+4) calculate the loss based on that distance, and optimize the model\r\n",
        "    ####    Label positive tuples as class '1', otherwise as class '0'\r\n",
        "    ####    You can also use sigmoid function rather than the softmax function.\r\n",
        "    ####    At the end, you must return the current loss value."
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xkdUFuHWdMT"
      },
      "source": [
        "for sample in tqdm(dataloader):\r\n",
        "  print(sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB5jT1Pklc06",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763,
          "referenced_widgets": [
            "f6b16456bec941559b01b2a8de44fb3d",
            "84c26ad7f63c4aea9f1630397f07318e",
            "867a5dd9b8ad47eb9cf4f380fcf8573a",
            "115a201a76074a35b8d5a4dbdff2eb08",
            "0469c4dafd7c4b4090ce67219e01db69",
            "ed3dae778cfe4422a3425117ca157a08",
            "c8278f4c0a5243a3afd0f8a3b37fb523",
            "aa4902a50b6e486e8ccea5648dfc7efe"
          ]
        },
        "outputId": "a7e00525-0fa0-498e-8151-2d77f02cf684"
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\r\n",
        "### PENALTY (-5) WILL BE GIVEN WHEN YOUR CODE RAISES AN ERROR DURING EPOCH.\r\n",
        "### YOUR TRAINING NEEDS TO BE PERFORMED WITH VARIOUS .TXT FILES.\r\n",
        "max_epoch = 1\r\n",
        "for epoch in range(max_epoch):\r\n",
        "    total_loss = 0.0\r\n",
        "    cnt = 0\r\n",
        "    for sample in tqdm(dataloader):\r\n",
        "        if cnt > 0:\r\n",
        "            curr_loss = train(model, optimizer, sample, prev_sample)\r\n",
        "            # print(curr_loss)\r\n",
        "            total_loss += curr_loss / len(dataloader)    \r\n",
        "        prev_sample = sample\r\n",
        "        cnt += 1\r\n",
        "        if cnt % 200 == 0:\r\n",
        "            print('[EPOCH {}] SAMPLED TRAIN LOSS : {}'.format(epoch, curr_loss))\r\n",
        "    print('[EPOCH {}] TOTAL LOSS : {}'.format(epoch, total_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6b16456bec941559b01b2a8de44fb3d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=118784.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[EPOCH 0] SAMPLED TRAIN LOSS : 0\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.418580805090203\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.5687522245232173\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3565224252565662\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4737497098978443\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4185573156508697\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.2859630149885704\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4737678016803568\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.610329479756149\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.2859700785630943\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3565425200587862\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3566035382019153\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3565829423646243\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4186088781679378\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4737436529529784\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.285911006173568\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 0\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3565567637676919\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3565727590859813\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3565527749569868\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 0\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.5235653195403238\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.418570810422215\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.5687838282744577\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4186134721627357\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.418570458758576\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.286000228858542\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3566475030503138\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 0\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4737417663598025\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.356570823830037\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.2859137267718992\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4185489700386396\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.5235224840916763\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4737517557232573\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.5687689908502962\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3565220451610753\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.2859725474377999\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.418564317712143\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.4737952337257418\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3566052952001793\n",
            "[EPOCH 0] SAMPLED TRAIN LOSS : 1.3565745460697944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsf3XQMbuGFE"
      },
      "source": [
        "### MUST TO DO 7 : (+10)\r\n",
        "##--------------write below-------------##\r\n",
        "# your code here\r\n",
        "##--------------write above-------------##\r\n",
        "## (+10) bring your word embedding as a numpy array \"embedding\"\r\n",
        "#### \"embedding\" should be N by D array, \r\n",
        "#### where N is the number of vocabularies, and D is the dimension of the word embedding. D is 300 in this code\r\n",
        "#### embedding[i, :] should be word embedding of dataset.vocabulary[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS8ukQiejMAI"
      },
      "source": [
        "### JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES. (check if visual makes sense , see graph in slides )\r\n",
        "reducer = PCA(n_components=2)\r\n",
        "# or try use\r\n",
        "# reducer = TSNE(n_components=2, verbose=1)\r\n",
        "reduce_results = reducer.fit_transform(embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTMFi8ydvKF8"
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\r\n",
        "top_k = 50\r\n",
        "sort_idx = np.argsort(list(dataset.frequency.values()))[::-1]\r\n",
        "sort_idx = sort_idx[:top_k]\r\n",
        "frequent_vocabs = [list(dataset.frequency.keys())[si] for si in sort_idx]\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "\r\n",
        "for idx, vocab in enumerate(dataset.vocabulary):\r\n",
        "    if vocab in frequent_vocabs:\r\n",
        "        plt.plot(reduce_results[idx, 0], reduce_results[idx, 1], '.')\r\n",
        "        plt.text(reduce_results[idx, 0], reduce_results[idx, 1], vocab)\\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFd4SQ5N9aRJ"
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\r\n",
        "min_dist = 10000000\r\n",
        "target_word = 'i'\r\n",
        "for idx, vocab in enumerate(dataset.vocabulary):\r\n",
        "    if vocab != target_word:\r\n",
        "        distance = np.linalg.norm(embedding[dataset.vocabulary.index(target_word)] - embedding[idx])\r\n",
        "        min_dist = min(distance, min_dist)\r\n",
        "        if min_dist == distance:\r\n",
        "            nearest_to_target = vocab\r\n",
        "print('\"{}\" is nearest to \"{}\"'.format(target_word, nearest_to_target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdLo52H62nDC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9lFo7at_1CV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}