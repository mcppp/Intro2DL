{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [SKELETON] Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcppp/Intro2DL/blob/main/Copy_of_%5BSKELETON%5D_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HplGJuJR01Rl"
      },
      "source": [
        "### USE ONLY THESE PACKAGES ###\r\n",
        "import os\r\n",
        "import csv\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrfsqemms0rN"
      },
      "source": [
        "### Upload the lyrics_dataset.zip in the colab's folder first !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MuSRKIVGyYR",
        "outputId": "c0758743-b7bb-443e-9587-7571036d1fcc"
      },
      "source": [
        "!unzip lyrics_dataset.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  lyrics_dataset.zip\n",
            "   creating: lyrics_dataset/\n",
            "  inflating: lyrics_dataset/adele.txt  \n",
            "  inflating: lyrics_dataset/al-green.txt  \n",
            "  inflating: lyrics_dataset/alicia-keys.txt  \n",
            "  inflating: lyrics_dataset/amy-winehouse.txt  \n",
            "  inflating: lyrics_dataset/beatles.txt  \n",
            "  inflating: lyrics_dataset/bieber.txt  \n",
            "  inflating: lyrics_dataset/bjork.txt  \n",
            "  inflating: lyrics_dataset/blink-182.txt  \n",
            "  inflating: lyrics_dataset/bob-dylan.txt  \n",
            "  inflating: lyrics_dataset/bob-marley.txt  \n",
            "  inflating: lyrics_dataset/britney-spears.txt  \n",
            "  inflating: lyrics_dataset/bruce-springsteen.txt  \n",
            "  inflating: lyrics_dataset/bruno-mars.txt  \n",
            "  inflating: lyrics_dataset/cake.txt  \n",
            "  inflating: lyrics_dataset/dickinson.txt  \n",
            "  inflating: lyrics_dataset/disney.txt  \n",
            "  inflating: lyrics_dataset/dj-khaled.txt  \n",
            "  inflating: lyrics_dataset/dolly-parton.txt  \n",
            "  inflating: lyrics_dataset/dr-seuss.txt  \n",
            "  inflating: lyrics_dataset/drake.txt  \n",
            "  inflating: lyrics_dataset/eminem.txt  \n",
            "  inflating: lyrics_dataset/janisjoplin.txt  \n",
            "  inflating: lyrics_dataset/jimi-hendrix.txt  \n",
            "  inflating: lyrics_dataset/johnny-cash.txt  \n",
            "  inflating: lyrics_dataset/joni-mitchell.txt  \n",
            "  inflating: lyrics_dataset/Kanye_West.txt  \n",
            "  inflating: lyrics_dataset/lady-gaga.txt  \n",
            "  inflating: lyrics_dataset/leonard-cohen.txt  \n",
            "  inflating: lyrics_dataset/Lil_Wayne.txt  \n",
            "  inflating: lyrics_dataset/lin-manuel-miranda.txt  \n",
            "  inflating: lyrics_dataset/lorde.txt  \n",
            "  inflating: lyrics_dataset/ludacris.txt  \n",
            "  inflating: lyrics_dataset/michael-jackson.txt  \n",
            "  inflating: lyrics_dataset/missy-elliott.txt  \n",
            "  inflating: lyrics_dataset/nickelback.txt  \n",
            "  inflating: lyrics_dataset/nicki-minaj.txt  \n",
            "  inflating: lyrics_dataset/nirvana.txt  \n",
            "  inflating: lyrics_dataset/notorious-big.txt  \n",
            "  inflating: lyrics_dataset/nursery_rhymes.txt  \n",
            "  inflating: lyrics_dataset/patti-smith.txt  \n",
            "  inflating: lyrics_dataset/paul-simon.txt  \n",
            "  inflating: lyrics_dataset/prince.txt  \n",
            "  inflating: lyrics_dataset/r-kelly.txt  \n",
            "  inflating: lyrics_dataset/radiohead.txt  \n",
            "  inflating: lyrics_dataset/rihanna.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfJnnXc7O58m"
      },
      "source": [
        "### Do not change this seed number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcZM4dCTpfz7",
        "outputId": "f05efc4c-5cc3-4276-9c8d-7fc276979791"
      },
      "source": [
        "np.random.seed(1)\r\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f790f019be8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fUfhvwLvAMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2178f01-cc52-426e-fb61-4c056c6f3863"
      },
      "source": [
        "## Use to construct dataset and dataloader!!!!\r\n",
        "## Use below code to train your model with all lyrics\r\n",
        "lyrics = list()\r\n",
        "for txt_file in os.listdir('./lyrics_dataset'):\r\n",
        "    if txt_file[0] != '.':\r\n",
        "        target_txt = os.path.join('./lyrics_dataset', txt_file)\r\n",
        "        f = open(target_txt, 'r')\r\n",
        "        curr_lyrics = f.readlines()\r\n",
        "        for i in range(len(curr_lyrics)):\r\n",
        "            curr_lyrics[i] = curr_lyrics[i].lower()\r\n",
        "        curr_lyrics = list(set(curr_lyrics))\r\n",
        "        lyrics += curr_lyrics\r\n",
        "# print(len(lyrics)) #how many lines\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#1: deleting everything but alphanumerics and blank spaces\r\n",
        "lyrics1=lyrics[:3]  #for short example\r\n",
        "stripped = []\r\n",
        "for i in lyrics1:\r\n",
        "  stripped.append ( ''.join(ch for ch in i if ch.isalnum() or ch == ' ').split())\r\n",
        "\r\n",
        "a = list()\r\n",
        "a = stripped\r\n",
        "\r\n",
        "type(a[0]) #checking that self.trimmed_lyrics is a list with lists inside of it\r\n",
        "# print(a)\r\n",
        "# print(a[0])\r\n",
        "\r\n",
        "\r\n",
        "#2: making vocabulary of unique words\r\n",
        "unique = list()\r\n",
        "for x in a:  \r\n",
        "    for i in x:\r\n",
        "      if i not in unique:\r\n",
        "        unique.append(i)\r\n",
        "# print(unique)\r\n",
        "\r\n",
        "\r\n",
        "##3: freq dict\r\n",
        "frequency = dict()\r\n",
        "for x in a:\r\n",
        "  for i in x:\r\n",
        "      if i not in frequency:\r\n",
        "        frequency[i] = 0 \r\n",
        "      frequency[i] += 1\r\n",
        "# print(frequency)\r\n",
        "\r\n",
        "##4:\r\n",
        "idx=0 #which sentence we want\r\n",
        "target_lyrics = a[idx] #chosen sentence separated in words\r\n",
        "print(target_lyrics)\r\n",
        "# print(len(target_lyrics))\r\n",
        "\r\n",
        "random_iterator = iter(target_lyrics) #creating iterator\r\n",
        "target_word = next(random_iterator) #randomly choose target word in sentence\r\n",
        "# print(target_word)\r\n",
        "target_idx = target_lyrics.index(target_word) #index of target word\r\n",
        "# print(target_idx)\r\n",
        "\r\n",
        "chosen_window = 3\r\n",
        "window_len = list(range(1, chosen_window+1, 1))\r\n",
        "print(window_len)\r\n",
        "\r\n",
        "sample = dict()\r\n",
        "sample['pairs'] = list()\r\n",
        "\r\n",
        "for i in window_len:\r\n",
        "  if (target_idx+i)<=len_word:\r\n",
        "    sample['pairs'].append([target_idx,target_idx + i])\r\n",
        "  if (target_idx - i) >= 0:\r\n",
        "    sample['pairs'].append([target_idx, target_idx - i])\r\n",
        "\r\n",
        "print(sample['pairs'])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# sample['pairs'][0] = (2,1)\r\n",
        "\r\n",
        "# target_idx =1\r\n",
        "# target_word = target_lyrics[target_idx]\r\n",
        "# print(target_word)\r\n",
        "\r\n",
        "\r\n",
        " \r\n",
        "##--------------write above-------------##\r\n",
        "        # Bring out one list, from self.trimmed_lyrics. (i.e., self.trimmed_lyrics[idx])\r\n",
        "        # Then, the list should contain the splited sentence in each lyric.\r\n",
        "        ### i.e., self.trimmed_lyrics[idx] would look like ['hey', 'nice', 'to', 'meet', 'you']\r\n",
        "        # (+1) randomly select the target word from self.trimmed_lyrics[idx]\r\n",
        "        # (+4) based on selected target word, and self.window_len,\r\n",
        "        #      generate a tuple of (target word index, context word index),\r\n",
        "        #      and add that tuple into sample['pairs']\r\n",
        "        ### if 'nice' is randomly chosen as the target element, and self.window_len = 1\r\n",
        "        ### if self.vocabulary[100] == 'hey', \r\n",
        "        ###    self.vocabulary[500] == 'nice',\r\n",
        "        ###    self.vocabulary[300] == 'to'\r\n",
        "        ### sample['pairs'] should be [(500, 300), (500, 100)]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "## Use below code to train your model with specific artist's lyrics\r\n",
        "# target_txt = './lyrics_dataset/lady-gaga.txt'\r\n",
        "# f = open(target_txt, 'r')\r\n",
        "# lyrics = f.readlines()\r\n",
        "# for i in range(len(lyrics)):\r\n",
        "#     lyrics[i] = lyrics[i].lower()\r\n",
        "# lyrics = list(set(lyrics))\r\n",
        "\r\n",
        "# # #\"lyrics\" is a list of strings.\r\n",
        "# for l in lyrics:\r\n",
        "#     print(l)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['groove', 'in', 'the', 'name', 'of', 'love', 'listen', 'love', 'brought', 'us', 'together']\n",
            "11\n",
            "groove\n",
            "0\n",
            "[1, 2, 3]\n",
            "[[3, 4], [3, 2], [3, 5], [3, 1], [3, 6], [3, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuH1d2SKvjmZ",
        "outputId": "b82b67aa-0488-457c-b367-6de1ddf5ca7b"
      },
      "source": [
        "#HELP EXAMPLES\r\n",
        "\r\n",
        "print(lyrics[103])\r\n",
        "trimmed_lyrics = list()\r\n",
        "trimmed_lyrics.append(['chola', 'or', 'orient', 'made'])\r\n",
        "\r\n",
        "trimmed_lyrics.append(['me', 'or', 'you'])\r\n",
        "# print(trimmed_lyrics) #how we want it to look like (eliminate symbols too)\r\n",
        "# vocabulary= ['chola', 'or', 'orient', 'made','me', 'you'] #vocab list with unique words\r\n",
        "\r\n",
        "# frequency=dict() #how many times a word appears\r\n",
        "# frequency['chola'] = 1\r\n",
        "# frequency['or'] = 2\r\n",
        "# frequency['orient'] = 1\r\n",
        "# frequency['made'] = 1\r\n",
        "# frequency['me'] = 1\r\n",
        "# frequency['you'] = 1\r\n",
        "# print(frequency)\r\n",
        "\r\n",
        "#MUST DO 4\r\n",
        "print(trimmed_lyrics[0])  #first list of words\r\n",
        "target_lyrics = trimmed_lyrics[0]\r\n",
        "targed_idx =1\r\n",
        "target_word = target_lyrics[target_idx]\r\n",
        "print(target_word)\r\n",
        "vocabulary= ['chola', 'or', 'orient', 'made','me', 'you'] #vocab list with unique words\r\n",
        "    #          0        1       2        3     4     5\r\n",
        "sample = dict()\r\n",
        "sample['pairs'] = list()\r\n",
        "sample['pairs'] = [(1,2),(1,0)] #[('or','orient'),('or', 'chola')] #  needs to be converted into integer\r\n",
        "                 #(targe word's index, context word's index)\r\n",
        "#your code here\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chola or orient made\n",
            "\n",
            "['chola', 'or', 'orient', 'made']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGJNDVzh9Bny"
      },
      "source": [
        "#we wanna get lyrics and eliminate commas, brackets, etc.\r\n",
        "\r\n",
        "class lyricsDataset(Dataset): # 12 points\r\n",
        "    def __init__(self, lyrics, window_len):\r\n",
        "        self.lyrics = lyrics\r\n",
        "        self.window_len = window_len #how many words we see before and after target word\r\n",
        "        # window len means, that, if i-th word is target\r\n",
        "        # you would consider i-window_len ~ i+window_len words as context words\r\n",
        "\r\n",
        "        ##--------------write below-------------## <--- do not erase this afterwards\r\n",
        "        self.trimmed_lyrics = list()    \r\n",
        "        self.vocabulary = list()\r\n",
        "        self.frequency = dict()\r\n",
        "        # your code in here\r\n",
        "        \r\n",
        "        #1: Eliminating everything but alphanumerics and blank spaces + splitting by blank spaces\r\n",
        "        stripped = []\r\n",
        "        for i in lyrics:\r\n",
        "          stripped.append ( ''.join(ch for ch in i if ch.isalnum() or ch == ' ').split())\r\n",
        "        self.trimmed_lyrics = stripped\r\n",
        "\r\n",
        "        #2: Making vocabulary of unique words\r\n",
        "        unique = list()\r\n",
        "        for x in self.trimmed_lyrics:  \r\n",
        "          for i in x:\r\n",
        "            if i not in unique:\r\n",
        "              unique.append(i)\r\n",
        "        self.vocabulary = unique\r\n",
        "\r\n",
        "        #3: Creating frequency dict\r\n",
        "        for x in self.trimmed_lyrics:\r\n",
        "          for i in x:\r\n",
        "            if i not in self.frequency:\r\n",
        "              self.frequency[i] = 0 \r\n",
        "            self.frequency[i] += 1\r\n",
        "        \r\n",
        "    \r\n",
        "        ##--------------write above-------------## <--- do not erase this\r\n",
        "        \r\n",
        "        ### MUST TO-DO 1. (+3) -> self.trimmed_lyrics\r\n",
        "        # for strings in self.lyrics, \r\n",
        "        # (+2) write a code to eliminate every character except for alphabet, number, and space (\" \")\r\n",
        "        ### for example, - + ? ! ' \" [ ] ( ) <- char like this should be excluded. \r\n",
        "        # (+1) after that, split each string with respect to the space.\r\n",
        "        ### If proprocessed string is \"hello hello hello\", \r\n",
        "        ### a list ['hello', 'hello', 'hello'] should be generated.\r\n",
        "        ### Then, put that list into the self.trimmed_lyrics.\r\n",
        "        ### self.trimmed_lyrics needs to be a LIST which has LIST as element.\r\n",
        "\r\n",
        "        ### MUST TO-DO 2. (+2) -> self.vocabulary\r\n",
        "        # (+2) Put all words(string) in self.trimmed_lyrics to the self.vocabulary. \r\n",
        "        ### In self.vocabulary, each word needs to be unique.\r\n",
        "        ### which means, this list (self.vocabulary) should not have duplicated elements. \r\n",
        "        ### self.vocabulary needs to be a LIST which contains unique words in self.trimmed_lyrics\r\n",
        "        ### If your code contains duplicated words, you would not get the point.\r\n",
        "        ### If there is a word that are neglected from self.trimmed_lyrics, you would not get the point.\r\n",
        "\r\n",
        "        ### MUST TO-DO 3. (+2) -> self.frequency\r\n",
        "        # (+2) In self.frequency, Record how many times each word in self.vocabulary \r\n",
        "        ###                              appears in self.trimmed_lyrics.\r\n",
        "        ### For example, if \"love\" appears 100 times in self.trimmed_lyrics,\r\n",
        "        ### it should be: self.frequency[\"love\"] = 100\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        # DO NOT TOUCH BELOW. JUST USE BELOW CODE FOR YOUR __len__ \r\n",
        "        return len(self.trimmed_lyrics)\r\n",
        "        # DO NOT TOUCH ABOVE. JUST USE ABOVE CODE FOR YOUR __len__ \r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        ### MUST TO-DO 4. (+5) --> sample (bring out positive samples from lyrics inside this dataset)\r\n",
        "        ##--------------write below-------------##\r\n",
        "        sample = dict()\r\n",
        "        sample['pairs'] = list()\r\n",
        "        # your code here\r\n",
        "        random_iterator = iter(self.trimmed_lyrics[idx]) #creating iterator\r\n",
        "        target_word = next(random_iterator) #randomly choose target word in sentence\r\n",
        "        target_idx = self.trimmed_lyrics[idx].index(target_word) #index of target word  \r\n",
        "        \r\n",
        "        window_len = list(range(1, self.window_len+1, 1)) #range to check if pair can be made\r\n",
        "\r\n",
        "        for i in window_len:\r\n",
        "          if (target_idx+i)<=len_word:\r\n",
        "            sample['pairs'].append([target_idx,target_idx + i])\r\n",
        "          if (target_idx - i) >= 0:\r\n",
        "            sample['pairs'].append([target_idx, target_idx - i])\r\n",
        "\r\n",
        "        return sample\r\n",
        "        ##--------------write above-------------##\r\n",
        "        # Bring out one list, from self.trimmed_lyrics. (i.e., self.trimmed_lyrics[idx])\r\n",
        "        # Then, the list should contain the splited sentence in each lyric.\r\n",
        "        ### i.e., self.trimmed_lyrics[idx] would look like ['hey', 'nice', 'to', 'meet', 'you']\r\n",
        "        # (+1) randomly select the target word from self.trimmed_lyrics[idx]\r\n",
        "        # (+4) based on selected target word, and self.window_len,\r\n",
        "        #      generate a tuple of (target word index, context word index),\r\n",
        "        #      and add that tuple into sample['pairs']\r\n",
        "        ### if 'nice' is randomly chosen as the target element, and self.window_len = 1\r\n",
        "        ### if self.vocabulary[100] == 'hey', \r\n",
        "        ###    self.vocabulary[500] == 'nice',\r\n",
        "        ###    self.vocabulary[300] == 'to'\r\n",
        "        ### sample['pairs'] should be [(500, 300), (500, 100)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzSkbpoFQnv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "633dc53b-b231-4abd-a840-6f6a1a8c6be5"
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES.\r\n",
        "### PENALTY (-5) CAN BE APPLIED IF YOUR CODE DOES NOT WORK FOR VARIOUS VALUES OF WINDOW_LEN\r\n",
        "dataset = lyricsDataset(lyrics, 3)\r\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\r\n",
        "### DO NOT TOUCH ABOVE. JUST USE THESE LINES."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeBtmoaymR0o",
        "outputId": "6827f889-34d2-49c7-d90b-fd1c6ac6f820"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118784"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5WbDFRf9yzI"
      },
      "source": [
        "class Word2Vec(nn.Module): # 15 points\r\n",
        "    def __init__(self, num_vocabs, embed_dim = 300): ## do not change 'embed_dim' value.\r\n",
        "        ### MUST TO-DO 5 : (+5)\r\n",
        "        ##--------------write below-------------##\r\n",
        "        # your code here\r\n",
        "        ##--------------write above-------------##\r\n",
        "        # Define a model which can map word's integer index value to word embedding.\r\n",
        "        \r\n",
        "    def forward(self, pairs):\r\n",
        "        '''\r\n",
        "        input : pairs - one tuple of (target word index, context word index)\r\n",
        "        '''\r\n",
        "        ### MUST TO DO 6 : (+10)\r\n",
        "        ##--------------write below-------------##\r\n",
        "        # your code here\r\n",
        "        ##--------------write above-------------##\r\n",
        "        # return the embedding of target word and context word.\r\n",
        "        # i.e., return target_embed, context_embed\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_djd4UufFd9"
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK WITH THESE LINES\r\n",
        "model = Word2Vec(len(dataset.vocabulary)) #the arg is num_vocabulary\r\n",
        "optimizer = torch.optim.AdamW(model.parameters())\r\n",
        "device = 'cuda'\r\n",
        "### DO NOT TOUCH ABOVE. JUST USE THESE LINES. MAKE YOUR CODE WORK WITH THESE LINES\r\n",
        "### USE THIS GIVEN OPTIMIZER."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rp8hvPWjHxe"
      },
      "source": [
        "def train(model, optimizer, sample_1, sample_2): # 13 points\r\n",
        "    ### MUST TO DO 6 : (+12)\r\n",
        "    ##--------------write below-------------##\r\n",
        "    model.train()\r\n",
        "    model = model.to(device)\r\n",
        "    optimizer.zero_grad()\r\n",
        "    # your code here\r\n",
        "    optimizer.step()\r\n",
        "    # return the current loss value\r\n",
        "    ##--------------write above-------------##\r\n",
        "\r\n",
        "    ## sample_1 will contain positive (target_1, context_1) tuples\r\n",
        "    ## sample_2 will contain another positive (target_2, context_2) tuples\r\n",
        "    ## But for the negative sampling, we need negative (target, context) tuples.\r\n",
        "    ## NEGATIVE TUPLES can be generated by (target_1, context_2), (target_2, context_1)\r\n",
        "    ## (+5) Generate and give positive & negative tuples for model's input\r\n",
        "    ##### Then, you would get : target_embed, context_embed = model((target, context)).\r\n",
        "    ## (+4) calculate the distance between target_embed and context_embed by DOT PRODUCT  #see slides\r\n",
        "    ## (+4) calculate the loss based on that distance, and optimize the model\r\n",
        "    ####    Label positive tuples as class '1', otherwise as class '0'\r\n",
        "    ####    You can also use sigmoid function rather than the softmax function.\r\n",
        "    ####    At the end, you must return the current loss value."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB5jT1Pklc06",
        "collapsed": true
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\r\n",
        "### PENALTY (-5) WILL BE GIVEN WHEN YOUR CODE RAISES AN ERROR DURING EPOCH.\r\n",
        "### YOUR TRAINING NEEDS TO BE PERFORMED WITH VARIOUS .TXT FILES.\r\n",
        "max_epoch = 1\r\n",
        "for epoch in range(max_epoch):\r\n",
        "    total_loss = 0.0\r\n",
        "    cnt = 0\r\n",
        "    for sample in tqdm(dataloader):\r\n",
        "        if cnt > 0:\r\n",
        "            curr_loss = train(model, optimizer, sample, prev_sample)\r\n",
        "            total_loss += curr_loss / len(dataloader)    \r\n",
        "        prev_sample = sample\r\n",
        "        cnt += 1\r\n",
        "        if cnt % 200 == 0:\r\n",
        "            print('[EPOCH {}] SAMPLED TRAIN LOSS : {}'.format(epoch, curr_loss))\r\n",
        "    print('[EPOCH {}] TOTAL LOSS : {}'.format(epoch, total_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsf3XQMbuGFE"
      },
      "source": [
        "### MUST TO DO 7 : (+10)\r\n",
        "##--------------write below-------------##\r\n",
        "# your code here\r\n",
        "##--------------write above-------------##\r\n",
        "## (+10) bring your word embedding as a numpy array \"embedding\"\r\n",
        "#### \"embedding\" should be N by D array, \r\n",
        "#### where N is the number of vocabularies, and D is the dimension of the word embedding. D is 300 in this code\r\n",
        "#### embedding[i, :] should be word embedding of dataset.vocabulary[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS8ukQiejMAI"
      },
      "source": [
        "### JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES. (check if visual makes sense , see graph in slides )\r\n",
        "reducer = PCA(n_components=2)\r\n",
        "# or try use\r\n",
        "# reducer = TSNE(n_components=2, verbose=1)\r\n",
        "reduce_results = reducer.fit_transform(embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTMFi8ydvKF8"
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\r\n",
        "top_k = 50\r\n",
        "sort_idx = np.argsort(list(dataset.frequency.values()))[::-1]\r\n",
        "sort_idx = sort_idx[:top_k]\r\n",
        "frequent_vocabs = [list(dataset.frequency.keys())[si] for si in sort_idx]\r\n",
        "plt.figure(figsize=(10, 6))\r\n",
        "\r\n",
        "for idx, vocab in enumerate(dataset.vocabulary):\r\n",
        "    if vocab in frequent_vocabs:\r\n",
        "        plt.plot(reduce_results[idx, 0], reduce_results[idx, 1], '.')\r\n",
        "        plt.text(reduce_results[idx, 0], reduce_results[idx, 1], vocab)\\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFd4SQ5N9aRJ"
      },
      "source": [
        "### DO NOT TOUCH BELOW. JUST USE THESE LINES. MAKE YOUR CODE WORK THESE CODES.\r\n",
        "min_dist = 10000000\r\n",
        "target_word = 'i'\r\n",
        "for idx, vocab in enumerate(dataset.vocabulary):\r\n",
        "    if vocab != target_word:\r\n",
        "        distance = np.linalg.norm(embedding[dataset.vocabulary.index(target_word)] - embedding[idx])\r\n",
        "        min_dist = min(distance, min_dist)\r\n",
        "        if min_dist == distance:\r\n",
        "            nearest_to_target = vocab\r\n",
        "print('\"{}\" is nearest to \"{}\"'.format(target_word, nearest_to_target))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdLo52H62nDC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9lFo7at_1CV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}